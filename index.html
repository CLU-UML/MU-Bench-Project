<!DOCTYPE html>
<html>
<head>
<APM_DO_NOT_TOUCH>

<script type="text/javascript">
(function(){
window.lde=!!window.lde;try{(function(){(function(){var a={decrypt:function(d){try{return JSON.parse(function(g){g=g.split("l");var h="";for(var k=0;k<g.length;++k)h+=String.fromCharCode(g[k]);return h}(d))}catch(g){}}};return a={configuration:a.decrypt("123l34l97l99l116l105l118l101l34l58l34l110l111l34l44l34l100l101l98l117l103l103l105l110l103l34l58l34l110l111l34l44l34l109l111l100l117l108l101l49l34l58l34l101l110l97l98l108l101l100l34l44l34l109l111l100l117l108l101l50l34l58l34l101l110l97l98l108l101l100l34l44l34l109l111l100l117l108l101l51l34l58l34l101l110l97l98l108l101l100l34l44l34l109l111l100l117l108l101l52l34l58l34l101l110l97l98l108l101l100l34l125")}})();
var b=69;try{var ba,da,ka=c(107)?1:0,oa=c(135)?1:0,qa=c(348)?1:0,ra=c(658)?0:1,sa=c(715)?0:1,va=c(268)?1:0,wa=c(159)?1:0,Ba=c(53)?1:0,Ca=c(328)?1:0;for(var Da=(c(902),0);Da<da;++Da)ka+=(c(302),2),oa+=c(936)?1:2,qa+=(c(55),2),ra+=(c(316),2),sa+=(c(125),2),va+=c(323)?2:1,wa+=(c(727),2),Ba+=(c(14),2),Ca+=(c(343),3);ba=ka+oa+qa+ra+sa+va+wa+Ba+Ca;window.Pa===ba&&(window.Pa=++ba)}catch(a){window.Pa=ba}var e=!0;function f(a,d){a+=d;return a.toString(36)}
function Ga(a){var d=3;a&&(document[q(d,121,108,118,108,101,108,111,108,119,124,86,119,100,119,104)]&&document[q(d,121,108,118,108,101,108,111,108,119,124,86,119,100,119,104)]!==f(68616527663,d)||(e=!1));return e}function t(a){var d=arguments.length,g=[];for(var h=1;h<d;++h)g.push(arguments[h]-a);return String.fromCharCode.apply(String,g)}function Ia(){}Ga(window[Ia[f(1086785,b)]]===Ia);Ga(typeof ie9rgb4!==f(1242178186130,b));
Ga(RegExp("\x3c")[f(1372136,b)](function(){return"\x3c"})&!RegExp(f(42820,b))[f(1372136,b)](function(){return"'x3'+'d';"}));
var Ja=window[q(b,166,185,185,166,168,173,138,187,170,179,185)]||RegExp(q(b,178,180,167,174,193,166,179,169,183,180,174,169),f(-51,b))[f(1372136,b)](window["\x6e\x61vi\x67a\x74\x6f\x72"]["\x75\x73e\x72A\x67\x65\x6et"]),Ka=+new Date+(c(250)?6E5:404816),Ma,Oa,Ra,Ta=window[t(b,184,170,185,153,174,178,170,180,186,185)],Ua=Ja?c(389)?3E4:20460:c(121)?6E3:6069;
document[t(b,166,169,169,138,187,170,179,185,145,174,184,185,170,179,170,183)]&&document[t(b,166,169,169,138,187,170,179,185,145,174,184,185,170,179,170,183)](q(b,187,174,184,174,167,174,177,174,185,190,168,173,166,179,172,170),function(a){var d=29;document[q(d,147,134,144,134,127,134,137,134,145,150,112,145,126,145,130)]&&(document[t(d,147,134,144,134,127,134,137,134,145,150,112,145,126,145,130)]===f(1058781954,d)&&a[q(d,134,144,113,143,146,144,145,130,129)]?Ra=!0:document[t(d,147,134,144,134,127,
134,137,134,145,150,112,145,126,145,130)]===f(68616527637,d)&&(Ma=+new Date,Ra=!1,A()))});function q(a){var d=arguments.length,g=[];for(var h=1;h<d;h++)g[h-1]=arguments[h]-a;return String.fromCharCode.apply(String,g)}function A(){if(!document[t(42,155,159,143,156,163,125,143,150,143,141,158,153,156)])return!0;var a=+new Date;if(a>Ka&&(c(796)?489498:6E5)>a-Ma)return Ga(!1);var d=Ga(Oa&&!Ra&&Ma+Ua<a);Ma=a;Oa||(Oa=!0,Ta(function(){Oa=!1},c(339)?1:0));return d}A();
var Xa=[c(700)?16657386:17795081,c(37)?27611931586:2147483647,c(250)?1558153217:1781051948];function $a(a){var d=44;a=typeof a===q(d,159,160,158,149,154,147)?a:a[t(d,160,155,127,160,158,149,154,147)](c(139)?36:43);var g=window[a];if(!g||!g[q(d,160,155,127,160,158,149,154,147)])return;var h=""+g;window[a]=function(k,l){Oa=!1;return g(k,l)};window[a][t(d,160,155,127,160,158,149,154,147)]=function(){return h}}for(var Ca=(c(115),0);Ca<Xa[f(1294399136,b)];++Ca)$a(Xa[Ca]);Ga(!1!==window[f(27629,b)]);
window.Ea=window.Ea||{};window.Ea.Ub="081e8163bc194000e34a2e10973332a8638e37c80da247286affe43f8d2d11b4a08484c449f4d4e37a0e74dce3d3b636c9cde28d9252b101588e059b7f89012a437047751de480d7";function B(a){var d=+new Date;if(!document[q(70,183,187,171,184,191,153,171,178,171,169,186,181,184,135,178,178)]||d>Ka&&(c(283)?6E5:661374)>d-Ma)var g=Ga(!1);else g=Ga(Oa&&!Ra&&Ma+Ua<d),Ma=d,Oa||(Oa=!0,Ta(function(){Oa=!1},c(233)?1:0));return!(arguments[a]^g)}function c(a){return 517>a}(function ab(a){a&&"number"!==typeof a||("number"!==typeof a&&(a=1E3),a=Math.max(a,1),setInterval(function(){ab(a-10)},a))})(!0);})();}catch(x){}finally{ie9rgb4=void(0);};function ie9rgb4(a,b){return a>>b>>0};

})();

</script>
</APM_DO_NOT_TOUCH>

<script type="text/javascript" src="/TSPD/082149a1b4ab20008a7479b6296311ee0566a292cc60fa9eba3405c588211dba0b8bde0609ffdcfa?type=9"></script>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We propose the first comprehensive multi-task multi-modal benchmark for Machine Unlearning.">
  <meta property="og:title" content="MU-Bench: A Multitask Multimodal Benchmark<br>for Machine Unlearning"/>
  <meta property="og:description" content="We uncover a vulnerability in LoRA fine-tuned models wherein an attacker is able to undo the fine-tuning process and recover the weights of the original pre-trained model."/>
  <meta property="og:url" content="https://vision.huji.ac.il/spectral_detuning/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/og_banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="MU-Bench: A Multitask Multimodal Benchmark for Machine Unlearning">
  <meta name="twitter:description" content="We uncover a vulnerability in LoRA fine-tuned models wherein an attacker is able to undo the fine-tuning process and recover the weights of the original pre-trained model.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/twitter_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Machine Unlearning, Benchmark, Evaluation, Standardize, Multitask, Multimodal, Vision Transformer, LLMs, Stable Diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MU-Bench</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="static/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon-16x16.png">
  <link rel="manifest" href="static/images/site.webmanifest">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MU-Bench: A Multitask Multimodal Benchmark for Machine Unlearning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://chengjiali.github.io/" target="_blank">Jiali Cheng</a>,</span>
                <span class="author-block">
                  <a href="https://cs.uml.edu/~hadi/" target="_blank">Hadi Amiri</a></span>
                  </div>

                <div class="is-size-5 publication-authors">
                  <span class="author-block">University of Massachusetts Lowell<br>SafeGenAI @ NeurIPS 2024<br></span>
                </div>

                <div class="column has-text-centered">
                  <div class="publication-links">
                   <!-- Arxiv PDF link -->
                   <span class="link-block">
                    <a href="https://arxiv.org/pdf/2406.14796" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>


                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2406.14796" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/CLU-UML/MU-Bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                  <!-- Dataset -->
                  <span class="link-block">
                      <a href="https://huggingface.co/collections/jialicheng/benchmark-664a3dd153317bdff3d2fe45" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon" style="vertical-align: middle; font-size: 20px;">&#129303;</span>
                          <span style="vertical-align: middle;">Dataset & Base Models</span>
                      </a>
                  </span>

                  <!-- Leaderboard -->
                  <span class="link-block">
                      <a href="https://huggingface.co/spaces/jialicheng/MU-Bench_Leaderboard" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fas fa-trophy"></i>
                          </span>
                          <span>Leaderboard</span>
                      </a>
                  </span>
                </div>
                <div class="is-size-5" style="margin-top: 20px;">
                  To add a new dataset, please fill out the 
                  <a href="https://docs.google.com/forms/d/e/1FAIpQLSfvCNaMy8H0-akM7DT4VoVOxLN_Qtd-wFre-EEYAPiCKC82xA/viewform?usp=header" target="_blank">Google Form</a>.
                </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p> Recent advancements in Machine Unlearning (MU) have introduced solutions to selectively remove certain training samples, such as those with outdated or sensitive information, from trained models. Despite these advancements, evaluation of MU methods have been inconsistent, employing different trained models and architectures, and sample removal strategies, which hampers accurate comparison. In addition, prior MU approaches have mainly focused on singular tasks or modalities, which is not comprehensive. To address these limitations, we develop MU-Bench, the first comprehensive benchmark for MU that (i) unifies the sets of deleted samples and trained models, and (ii) provides broad coverage of tasks and data modalities, including previously unexplored domains such as speech and video classification. Our evaluation show that RandLabel and SalUn are the most effective general MU approaches on MU-Bench, and BadT and SCRUB are capable of achieving random performance on the deletion set. We analyze several under-investigated aspects of unlearning, including scalability, the impacts of parameter-efficient fine-tuning and curriculum learning, and susceptibility to dataset biases. MU-Bench provides an easy-to-use package that includes dataset splits, models, and implementations, together with a leader board to enable unified and scalable MU research.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <div class="level-set has-text-justified">
            <p>
             Prior evaluation on machine unlearning is conducted on different base models, deleted samples, and specific tasks.
              We benchmark machine unlearning by unifying 1 a vulnerability in fine-tuned models, wherein the pre-fine-tuning (Pre-FT) weights, i.e., the model weights before the fine
             <ol>
              <li>Deleted Samples.</li>
              <li>Base models.</li>
            </ol>
            Recovering the original pre-fine-tuning unsafe weights, is implicitly assumed to be impossible. We demonstrate that this safety assumption is often false.
          </p>
        </div>
<img src="static/images/mistral.png" alt="Recovering the Pre-Fine-Tuning Weight of an Aligned Mistral 7B" class="blend-img-background center-image"/>
        </div>
        
      </div>
    </div>
  </div>
</div>
</section>

<!-- Dataset Table -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <div class="level-set has-text-justified">
          <body>
              <table>
                  <caption>Dataset Overview</caption>
                  <thead>
                      <tr>
                          <th>Dataset</th>
                          <th>Task</th>
                          <th>Domain</th>
                          <th>Modality</th>
                          <th>D</th>
                      </tr>
                  </thead>
                  <tbody>
                      <tr>
                          <td class="bold">Discriminative Tasks</td>
                          <td></td>
                          <td></td>
                          <td></td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>CIFAR-100</td>
                          <td>Image Classification</td>
                          <td>General</td>
                          <td>Image</td>
                          <td>50K</td>
                      </tr>
                      <tr>
                          <td>IMDB</td>
                          <td>Sentiment Classification</td>
                          <td>Movie Review</td>
                          <td>Text</td>
                          <td>25K</td>
                      </tr>
                      <tr>
                          <td class="bold">DDI-2013</td>
                          <td>Relation Extraction</td>
                          <td>Biomedical</td>
                          <td>Text</td>
                          <td>25K</td>
                      </tr>
                      <tr>
                          <td>NLVR²</td>
                          <td>Visual Reasoning</td>
                          <td>General</td>
                          <td>Image-Image-Text</td>
                          <td>62K</td>
                      </tr>
                      <tr>
                          <td class="bold">Speech Commands</td>
                          <td>Keyword Spotting</td>
                          <td>Commands</td>
                          <td>Speech</td>
                          <td>85K</td>
                      </tr>
                      <tr>
                          <td class="bold">UCF101</td>
                          <td>Action Classification</td>
                          <td>General</td>
                          <td>Video</td>
                          <td>9.3K</td>
                      </tr>
                      <tr>
                          <td class="bold">Generative Tasks</td>
                          <td></td>
                          <td></td>
                          <td></td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>SAMSum</td>
                          <td>Text Summarization</td>
                          <td>Chat Dialogue</td>
                          <td>Text</td>
                          <td>14K</td>
                      </tr>
                      <tr>
                          <td class="bold">Celeb Profile</td>
                          <td>Text Generation</td>
                          <td>Biography</td>
                          <td>Text</td>
                          <td>183</td>
                      </tr>
                      <tr>
                          <td class="bold">Tiny ImageNet</td>
                          <td>Text-to-Image Generation</td>
                          <td>General</td>
                          <td>Image-Text</td>
                          <td>20K</td>
                      </tr>
                  </tbody>
              </table>
              <p style="text-align: center;"><strong>Bold</strong> datasets are ones that have never been evaluated in unlearning.</p>
          </body>
        
      </div>
    </div>
  </div>
</div>
</section>
<!-- End Dataset Table -->


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">A Unified View of Machine Unlearning Methods</h2>
      <div class="level-set has-text-justified">
        <p>
          We propose the task of <it>Pre-Fine-Tuning Weight Recovery</it>. In this paper, we tackle this task in cases where multiple LoRA fine-tuned flavors of the same source model are available.
          To solve this task we present <em>Spectral DeTuning</em>, a method that recovers Pre-FT weights of SoTA models using iterative low-rank matrix factorization
          </p>
       <p>
        Unlike previous attacks on model alignment that attempt to recover Pre-FT capabilities, we aim to recover the exact Pre-FT weights.<br>
        Moreover, it does not require running inference through the model. This is advantageous as i) it does not require training data ii) it is highly parallelizable, e.g., on a cluster of desktop GPUs such as RTX2080 our method can recover the Pre-FT weights of a Mistral-7B model in under five minutes.
       </p>
     </div>
     <img src="static/images/stable_diffusion.png" alt="Recovering the Pre-Fine-Tuning Weight of an Aligned Mistral 7B" class="blend-img-background center-image"/>
          <p><strong>Stable Diffusion Results:</strong> Spectral DeTuning recovers the Pre-Fine-Tuning images with high precision, even when using "in the wild" LoRAs, essentially reversing the personalization fine-tuning of the LoRA model.
      </div>
   </div>
 </div>
</section>



<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Vulnerability of SoTA Models</h2>
          <div class="level-set has-text-justified">
            <p>
              By using just 5 LoRAs taken from CivitAI, we can recover the Pre-FT Stable Diffusion weights with a vanishingly small error. As can be seen below, scaling up to a DPO aligned Mistral only requires 8 LoRAs.
            </p>
            <img src="static/images/semantic_conv.png" alt="Number of LoRAs for semantic convergence" class="center-image"/>
            <div class="container mt-4">
              <div class="alert alert-danger" role="alert">
                <strong>Implications:</strong> SoTA LLMs that use LoRA for alignment fine-tuning are vulnerable to Pre-FT weight recovery attacks
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">Spectral DeTuning</h2>
      <div class="level-set has-text-justified">
        <p>
          The core idea of Spectral DeTuning is to iteratively break down the optimization into a set of simple sub-problems which have closed-form solutions. This results in a simple yet powerful algorithm that can be implemented in 8 lines of code.
       </p>
       <img src="static/images/algorithm.png" alt="Spectral DeTuning code" class="center-image"/>
     </div>
   </div>
 </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">LoWRA Bench</h2>
          <div class="level-set has-text-justified">
            <p>
              To stimulate research into preventing Pre-FT weight leakage and the associated risks in terms of model safety and alignment we present <strong>Lo</strong>RA <strong>W</strong>eight <strong>R</strong>ecovery <strong>A</strong>ttack (LoWRA) Bench, a comprehensive benchmark designed to evaluate Pre-FT weight recovery methods.<br>

              Our dataset encompasses three pre-trained representative source models: a Vision Transformer (ViT) trained on ImageNet-1K, Stable Diffusion 1.5, and Mistral-7B-v0.1. Notably, these models are widely used and deployed in numerous production systems.

            </p>
            <img src="static/images/lowra_bench.png" alt="LoWRA Bench Details" class="center-image"/>
          </div>
        </div>
      </div>
    </div>
  </section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">Broader Impact</h2>
      <div class="level-set has-text-justified">
        <p>
          This work uncovers a significant vulnerability in fine-tuned models, allowing attackers to access pre-fine-tuning weights. While this discovery reveals potential security risks, our primary objective is to advance the field of Machine Learning and raise awareness within the research community about the existing vulnerabilities in current models.
        </p>
        <p>
          Instead of using the findings of this study to execute attacks, we advocate for their use by model creators to enhance the safety and security of their models. By acknowledging and addressing vulnerabilities, creators can proactively safeguard against potential threats.
        </p>
        <p>
          Furthermore, in the discussion section, we outline potential future directions and mitigation strategies. Following established practices in the cyber security community, we emphasize the importance of open discussion and encourage the reporting of vulnerabilities. By fostering transparency and collaboration, we can collectively create a safer environment for deploying machine learning models.
       </p>
     </div>
   </div>
 </div>
</section>



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{cheng2024mu,
      title={Mu-bench: A multitask multimodal benchmark for machine unlearning},
      author={Cheng, Jiali and Amiri, Hadi},
      journal={arXiv preprint arXiv:2406.14796},
      year={2024}
    }</code></pre>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theֲ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>ֲ project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- Default Statcounter code for Spectral DeTuning
https://vision.huji.ac.il/spectral_detuning/ -->
<script type="text/javascript">
var sc_project=12968013; 
var sc_invisible=1; 
var sc_security="c534194a"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics
Made Easy - Statcounter" href="https://statcounter.com/"
target="_blank"><img class="statcounter"
src="https://c.statcounter.com/12968013/0/c534194a/1/"
alt="Web Analytics Made Easy - Statcounter"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

</body>
</html>
